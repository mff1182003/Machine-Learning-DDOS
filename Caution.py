import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler # ThÃªm scaler náº¿u cáº§n
import matplotlib.pyplot as plt
import joblib
import time

# --- 1. HÃ m há»— trá»£ (TÆ°Æ¡ng tá»± nhÆ° code ARP trÆ°á»›c) ---

def add_noise_to_numerical_features(X, noise_factor=0.15, random_state=42):
    """ThÃªm nhiá»…u Gaussian vÃ o táº¥t cáº£ cÃ¡c features sá»‘."""
    np.random.seed(random_state)
    X_noisy = X.copy()
    numerical_cols = X_noisy.select_dtypes(include=np.number).columns.tolist()

    noise_applied_cols = []
    for col in numerical_cols:
        if X_noisy[col].std() > 1e-6: # Chá»‰ thÃªm nhiá»…u náº¿u cÃ³ Ä‘á»™ biáº¿n thiÃªn
            noise = np.random.normal(0, noise_factor * X_noisy[col].std(), size=len(X_noisy))
            X_noisy[col] = X_noisy[col] + noise
            noise_applied_cols.append(col)
    print(f"ğŸ² ÄÃ£ thÃªm nhiá»…u cho {len(noise_applied_cols)} features sá»‘: {noise_applied_cols}")
    return X_noisy

# --- 2. HÃ m chÃ­nh Ä‘á»ƒ huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ ---

def train_and_evaluate_model(labeled_csv_file, test_size=0.2, cv_folds=10, noise_factor=0.15, random_state=42):
    """
    Huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh RandomForestClassifier vá»›i Grid Search vÃ  ká»¹ thuáº­t chá»‘ng overfitting.
    """
    print("ğŸš€ Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh...")
    start_total_time = time.time()

    # --- BÆ°á»›c 1: Äá»c vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u cÆ¡ báº£n ---
    print("\n--- BÆ°á»›c 1: Äá»c vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u cÆ¡ báº£n ---")
    try:
        df = pd.read_csv(labeled_csv_file)
        print(f"ğŸ“Š ÄÃ£ Ä‘á»c {len(df)} máº«u dá»¯ liá»‡u tá»« '{labeled_csv_file}'")
    except FileNotFoundError:
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file '{labeled_csv_file}'. Vui lÃ²ng Ä‘áº£m báº£o file náº±m cÃ¹ng thÆ° má»¥c vá»›i script.")
        return

    # Xá»­ lÃ½ giÃ¡ trá»‹ vÃ´ háº¡n vÃ  NaN
    initial_rows = len(df)
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    df.dropna(inplace=True)
    if len(df) < initial_rows:
        print(f"âœ… ÄÃ£ xá»­ lÃ½ NaN/Inf: Loáº¡i bá» {initial_rows - len(df)} dÃ²ng. CÃ²n láº¡i {len(df)} dÃ²ng.")
    else:
        print("âœ… KhÃ´ng cÃ³ giÃ¡ trá»‹ NaN/Inf nÃ o Ä‘Æ°á»£c tÃ¬m tháº¥y vÃ  loáº¡i bá».")

    if 'Label' not in df.columns:
        print(f"âŒ Lá»—i: Thiáº¿u cá»™t 'Label' trong file '{labeled_csv_file}'. CÃ¡c cá»™t cÃ³ sáºµn: {df.columns.tolist()}")
        return

    # TÃ¡ch X vÃ  y
    X = df.drop("Label", axis=1)
    y = df["Label"]

    print(f"ğŸ” CÃ¡c cá»™t Feature (X): {X.columns.tolist()}")
    print(f"ğŸ¯ Cá»™t NhÃ£n (y): Label")

    # Kiá»ƒm tra phÃ¢n bá»‘ nhÃ£n
    print(f"\nğŸ“ˆ PhÃ¢n bá»‘ nhÃ£n trong toÃ n bá»™ táº­p dá»¯ liá»‡u: {dict(y.value_counts())}")
    if len(y.unique()) < 2:
        print("âš ï¸  Cáº£nh bÃ¡o: Chá»‰ cÃ³ má»™t lá»›p duy nháº¥t trong cá»™t 'Label'. MÃ´ hÃ¬nh sáº½ khÃ´ng thá»ƒ há»c phÃ¢n loáº¡i.")
        return
    elif y.value_counts().min() / y.value_counts().max() < 0.1: # Náº¿u lá»›p thiá»ƒu sá»‘ < 10% lá»›p Ä‘a sá»‘
        print("âš ï¸  Cáº£nh bÃ¡o: Dá»¯ liá»‡u bá»‹ máº¥t cÃ¢n báº±ng nghiÃªm trá»ng. HÃ£y chÃº Ã½ Ä‘áº¿n F1-score hÆ¡n Accuracy.")

    # ThÃªm nhiá»…u vÃ o features sá»‘ (Ã¡p dá»¥ng cho toÃ n bá»™ X trÆ°á»›c khi chia train/test Ä‘á»ƒ CV nháº¥t quÃ¡n)
    print(f"\nğŸ² ThÃªm nhiá»…u vÃ o features vá»›i há»‡ sá»‘ {noise_factor} Ä‘á»ƒ chá»‘ng overfitting...")
    X_noisy = add_noise_to_numerical_features(X, noise_factor=noise_factor, random_state=random_state)


    # --- BÆ°á»›c 2: Chia táº­p Train/Test vÃ  Preprocessing ---
    print("\n--- BÆ°á»›c 2: Chia táº­p Train/Test vÃ  Preprocessing ---")
    # Chia táº­p train vÃ  test má»™t cÃ¡ch stratify Ä‘á»ƒ giá»¯ tá»· lá»‡ nhÃ£n
    X_train, X_test, y_train, y_test = train_test_split(
        X_noisy, y, test_size=test_size, random_state=random_state, stratify=y
    )
    print(f"ğŸ“¦ Táº­p huáº¥n luyá»‡n: {len(X_train)} máº«u | Táº­p kiá»ƒm tra: {len(X_test)} máº«u")

    # XÃ¡c Ä‘á»‹nh cÃ¡c cá»™t sá»‘ (numerical) vÃ  cá»™t nhá»‹ phÃ¢n/phÃ¢n loáº¡i (náº¿u cÃ³)
    numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()
    # Náº¿u cÃ³ cá»™t cáº§n mÃ£ hÃ³a one-hot hoáº·c passthrough, báº¡n cÃ³ thá»ƒ Ä‘á»‹nh nghÄ©a á»Ÿ Ä‘Ã¢y
    # VÃ­ dá»¥: categorical_features = X_train.select_dtypes(include='object').columns.tolist()

    # Táº¡o pipeline vá»›i StandardScaler (chuáº©n hÃ³a dá»¯ liá»‡u sá»‘) vÃ  RandomForestClassifier
    # StandardScaler giÃºp cÃ¡c thuáº­t toÃ¡n khÃ¡c nháº¡y cáº£m vá»›i thang Ä‘o hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n
    # RandomForest Ã­t nháº¡y cáº£m, nhÆ°ng váº«n lÃ  practice tá»‘t náº¿u cÃ³ cÃ¡c thuáº­t toÃ¡n khÃ¡c trong pipeline
    preprocessing_pipeline = Pipeline(steps=[
        ('scaler', StandardScaler())
    ])

    # Pipeline Ä‘áº§y Ä‘á»§ bao gá»“m preprocessing vÃ  classifier
    # LÆ°u Ã½: GridSearchCV sáº½ tÃ¬m kiáº¿m trÃªn cÃ¡c tham sá»‘ cá»§a clf trong pipeline
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessing_pipeline),
        ('clf', RandomForestClassifier(
            n_estimators=100,
            random_state=random_state,
            class_weight='balanced' # CÃ¢n báº±ng class, há»¯u Ã­ch cho dá»¯ liá»‡u máº¥t cÃ¢n báº±ng
        ))
    ])

    # --- BÆ°á»›c 3: Grid Search Ä‘á»ƒ tÃ¬m max_depth tá»‘i Æ°u ---
    print("\n--- BÆ°á»›c 3: Grid Search Ä‘á»ƒ tÃ¬m max_depth tá»‘i Æ°u ---")
    # Thiáº¿t láº­p tham sá»‘ cho Grid Search
    param_grid = {
        'clf__max_depth': [3, 5, 7, 10, 15, 20, 25, 30] # ThÃªm cÃ¡c giÃ¡ trá»‹ nhá» hÆ¡n cho max_depth
    }

    # Stratified K-Fold cho Cross-validation trong Grid Search
    cv_strategy_gs = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)

    print(f"ğŸ” Báº¯t Ä‘áº§u Grid Search vá»›i {cv_folds}-Fold Cross-validation vÃ  scoring='f1_macro'...")
    grid_search = GridSearchCV(
        estimator=pipeline, # Sá»­ dá»¥ng pipeline lÃ m estimator
        param_grid=param_grid,
        scoring='f1_macro', # Sá»­ dá»¥ng F1-macro cho cÃ¢n báº±ng hÆ¡n, tá»‘t cho bÃ i toÃ¡n phÃ¢n loáº¡i
        cv=cv_strategy_gs,
        n_jobs=-1, # Sá»­ dá»¥ng táº¥t cáº£ cÃ¡c lÃµi CPU
        verbose=1 # In tiáº¿n Ä‘á»™
    )
    grid_search.fit(X_train, y_train)

    best_depth = grid_search.best_params_['clf__max_depth']
    print(f"ğŸ† Best max_depth Ä‘Æ°á»£c tÃ¬m tháº¥y: {best_depth}")
    print(f"ğŸ† Best F1-macro score tá»« Grid Search: {grid_search.best_score_:.4f}")

    # --- BÆ°á»›c 4: ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh vá»›i Best max_depth vÃ  Cross-validation ---
    print("\n--- BÆ°á»›c 4: ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh vá»›i Best max_depth vÃ  Cross-validation ---")

    # Thiáº¿t láº­p pipeline vá»›i max_depth tá»‘t nháº¥t
    final_pipeline = Pipeline(steps=[
        ('preprocessor', preprocessing_pipeline),
        ('clf', RandomForestClassifier(
            n_estimators=100,
            random_state=random_state,
            max_depth=best_depth, # Sá»­ dá»¥ng best_depth
            class_weight='balanced'
        ))
    ])

    # ÄÃ¡nh giÃ¡ láº¡i vá»›i cross_val_score trÃªn Táº¤T Cáº¢ Dá»® LIá»†U (X_noisy, y)
    # Ä‘á»ƒ cÃ³ cÃ¡i nhÃ¬n tá»•ng quan vá» hiá»‡u suáº¥t cá»§a cáº¥u hÃ¬nh tá»‘i Æ°u.
    # Sá»­ dá»¥ng cÃ¹ng chiáº¿n lÆ°á»£c CV nhÆ° Grid Search
    print(f"ğŸ“Š ÄÃ¡nh giÃ¡ cuá»‘i cÃ¹ng báº±ng {cv_folds}-Fold Cross-validation trÃªn toÃ n bá»™ dá»¯ liá»‡u...")
    cv_scores_final = cross_val_score(final_pipeline, X_noisy, y, cv=cv_strategy_gs, scoring='f1_macro', n_jobs=-1)
    print(f"ğŸ“Š F1-macro trung bÃ¬nh qua cÃ¡c folds: {cv_scores_final.mean():.4f} (+/- {cv_scores_final.std() * 2:.4f})")


    # --- BÆ°á»›c 5: Huáº¥n luyá»‡n mÃ´ hÃ¬nh cuá»‘i cÃ¹ng vÃ  ÄÃ¡nh giÃ¡ trÃªn táº­p test ---
    print("\n--- BÆ°á»›c 5: Huáº¥n luyá»‡n mÃ´ hÃ¬nh cuá»‘i cÃ¹ng vÃ  ÄÃ¡nh giÃ¡ trÃªn táº­p test ---")

    # Huáº¥n luyá»‡n mÃ´ hÃ¬nh cuá»‘i cÃ¹ng trÃªn toÃ n bá»™ táº­p huáº¥n luyá»‡n Ä‘Ã£ tá»‘i Æ°u
    start_train_time = time.time()
    final_pipeline.fit(X_train, y_train)
    training_time = time.time() - start_train_time
    print(f"âœ… Huáº¥n luyá»‡n mÃ´ hÃ¬nh cuá»‘i cÃ¹ng hoÃ n táº¥t sau {training_time:.2f} giÃ¢y.")

    # Dá»± Ä‘oÃ¡n vÃ  Ä‘Ã¡nh giÃ¡
    y_train_pred = final_pipeline.predict(X_train)
    y_test_pred = final_pipeline.predict(X_test)

    train_acc = accuracy_score(y_train, y_train_pred)
    test_acc = accuracy_score(y_test, y_test_pred)
    train_f1 = f1_score(y_train, y_train_pred, average='macro')
    test_f1 = f1_score(y_test, y_test_pred, average='macro')

    print(f"\nğŸ¯ Káº¾T QUáº¢ ÄÃNH GIÃ TRÃŠN Táº¬P HUáº¤N LUYá»†N VÃ€ KIá»‚M TRA:")
    print(f"   Training Accuracy: {train_acc:.4f}")
    print(f"   Test Accuracy: {test_acc:.4f}")
    print(f"   Training F1-score (macro): {train_f1:.4f}")
    print(f"   Test F1-score (macro): {test_f1:.4f}")
    print(f"   Accuracy Gap (Train - Test): {train_acc - test_acc:.4f}")
    print(f"   F1-score Gap (Train - Test): {train_f1 - test_f1:.4f}")

    if (train_acc - test_acc) > 0.05 or (train_f1 - test_f1) > 0.05: # NgÆ°á»¡ng 0.05 cho gap
        print("âš ï¸  Cáº¢NH BÃO: CÃ³ dáº¥u hiá»‡u overfitting! (Gap > 0.05)")
    else:
        print("âœ… Model cÃ³ váº» á»•n Ä‘á»‹nh, khÃ´ng overfitting nghiÃªm trá»ng.")

    print(f"\nğŸ“‹ Classification Report (Test Set):")
    print(classification_report(y_test, y_test_pred))

    # --- BÆ°á»›c 6: Hiá»ƒn thá»‹ Ma tráº­n nháº§m láº«n vÃ  Quan trá»ng cá»§a Features ---
    print("\n--- BÆ°á»›c 6: Hiá»ƒn thá»‹ Ma tráº­n nháº§m láº«n vÃ  Quan trá»ng cá»§a Features ---")
    cm = confusion_matrix(y_test, y_test_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=final_pipeline.classes_)
    disp.plot(cmap=plt.cm.Blues)
    plt.title("Confusion Matrix (Test Set)")
    plt.show()

    # Feature importance (chá»‰ tá»« classifier trong pipeline)
    if hasattr(final_pipeline.named_steps['clf'], 'feature_importances_'):
        importances = final_pipeline.named_steps['clf'].feature_importances_
        feature_imp = pd.DataFrame({
            'feature': X.columns,
            'importance': importances
        }).sort_values('importance', ascending=False)

        print(f"\nğŸ” TOP 10 FEATURES QUAN TRá»ŒNG NHáº¤T:")
        print(feature_imp.head(10))

    # --- BÆ°á»›c 7: LÆ°u mÃ´ hÃ¬nh ---
    print("\n--- BÆ°á»›c 7: LÆ°u mÃ´ hÃ¬nh ---")
    model_save_path = "model_cau.joblib"
    joblib.dump(final_pipeline, model_save_path)
    print(f"âœ… ÄÃ£ lÆ°u mÃ´ hÃ¬nh tá»‘i Æ°u: '{model_save_path}'")

    end_total_time = time.time()
    print(f"\nğŸ‰ ToÃ n bá»™ quÃ¡ trÃ¬nh hoÃ n táº¥t sau {end_total_time - start_total_time:.2f} giÃ¢y.")

# Cháº¡y script
if __name__ == "__main__":
    train_and_evaluate_model(
        labeled_csv_file='train.csv',
        test_size=0.2,
        cv_folds=10,
        noise_factor=0.05, # Giáº£m noise factor má»™t chÃºt, hoáº·c giá»¯ 0.15 náº¿u dá»¯ liá»‡u váº«n quÃ¡ separable
        random_state=42
    )